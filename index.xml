<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vision &amp; AI lab</title>
    <link>https://VisionAI-lab.github.io/</link>
      <atom:link href="https://VisionAI-lab.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Vision &amp; AI lab</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â©2024 JinkyuKim</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://VisionAI-lab.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Vision &amp; AI lab</title>
      <link>https://VisionAI-lab.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://VisionAI-lab.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ENTP: Encoder-only Next Token Prediction</title>
      <link>https://VisionAI-lab.github.io/publication/2024_arxiv_entp/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_arxiv_entp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>InstructBooth: Instruction-following Personalized Text-to-Image Generation</title>
      <link>https://VisionAI-lab.github.io/publication/2024_arxiv_instructbooth/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_arxiv_instructbooth/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse-to-Dense LiDAR Point Generation by LiDAR-Camera Fusion for 3D Object Detection</title>
      <link>https://VisionAI-lab.github.io/publication/2024_arxiv_fusion/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_arxiv_fusion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>H-Direct: Homeostasis-aware Direct Spike Encoding for Deep Spiking Neural Networks</title>
      <link>https://VisionAI-lab.github.io/publication/2024_neuripsw_snn/</link>
      <pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_neuripsw_snn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Just Add $100 More, Augmenting Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem</title>
      <link>https://VisionAI-lab.github.io/publication/2024_neurips_naver/</link>
      <pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_neurips_naver/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection</title>
      <link>https://VisionAI-lab.github.io/publication/2024_neurips_dg/</link>
      <pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_neurips_dg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bridging the Domain Gap by Clustering-based Image-Text Graph Matching</title>
      <link>https://VisionAI-lab.github.io/publication/2024_icpr_dg/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_icpr_dg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Text-Driven Prototype Learning for Few-Shot Class-Incremental Learning</title>
      <link>https://VisionAI-lab.github.io/publication/2024_icpr_cil/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_icpr_cil/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Who Should Have Been Focused: Transferring Attention-based Knowledge from Future Observations for Trajectory Prediction</title>
      <link>https://VisionAI-lab.github.io/publication/2024_icpr_bp/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_icpr_bp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Leveraging Inductive Bias in ViT for Medical Image Diagnosis</title>
      <link>https://VisionAI-lab.github.io/publication/2024_bmvc_medical/</link>
      <pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_bmvc_medical/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding</title>
      <link>https://VisionAI-lab.github.io/publication/2024_emnlpw_multi/</link>
      <pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_emnlpw_multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages</title>
      <link>https://VisionAI-lab.github.io/publication/2024_emnlp_multi/</link>
      <pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_emnlp_multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Finetuning Pre-trained Model with Limited Data for LiDAR-based 3D Object Detection by Bridging Domain Gaps</title>
      <link>https://VisionAI-lab.github.io/publication/2024_iros_dg/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_iros_dg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhanced Motion Forecasting with Visual Relation Reasoning</title>
      <link>https://VisionAI-lab.github.io/publication/2024_eccv_virr/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_eccv_virr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System</title>
      <link>https://VisionAI-lab.github.io/publication/2024_eccv_lrslam/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_eccv_lrslam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MEVG Multi-event Video Generation with Text-to-Video Models</title>
      <link>https://VisionAI-lab.github.io/publication/2024_eccv_mevg/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_eccv_mevg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions</title>
      <link>https://VisionAI-lab.github.io/publication/2024_eccv_visiontrap/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_eccv_visiontrap/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection</title>
      <link>https://VisionAI-lab.github.io/publication/2024_icra_3dod/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_icra_3dod/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Sound-guided Image Manipulation</title>
      <link>https://VisionAI-lab.github.io/publication/2024_nn_sound/</link>
      <pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_nn_sound/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EGTR: Extracting Graph from Transformer for Scene Graph Generation</title>
      <link>https://VisionAI-lab.github.io/publication/2024_cvpr_sgg/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_cvpr_sgg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Higher-order Relational Reasoning for Pedestrian Trajectory Prediction</title>
      <link>https://VisionAI-lab.github.io/publication/2024_cvpr_highgraph/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_cvpr_highgraph/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-based 3D Object Detection</title>
      <link>https://VisionAI-lab.github.io/publication/2024_aaai_cmda/</link>
      <pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_aaai_cmda/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BEVMap: Map-Aware BEV Modeling for 3D Perception</title>
      <link>https://VisionAI-lab.github.io/publication/2024_wacv_bevmap/</link>
      <pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_wacv_bevmap/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Localization and Manipulation of Immoral Visual Cues for Safe Text-to-Image Generation</title>
      <link>https://VisionAI-lab.github.io/publication/2024_wacv_ethics/</link>
      <pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2024_wacv_ethics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models</title>
      <link>https://VisionAI-lab.github.io/publication/cream_emnlp2023/</link>
      <pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/cream_emnlp2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distillation for High-Quality Knowledge Extraction via Explainable Oracle Approach</title>
      <link>https://VisionAI-lab.github.io/publication/xai_bmvc2023/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/xai_bmvc2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internship (Tuebingen AI Center)</title>
      <link>https://VisionAI-lab.github.io/post/tuebingen/</link>
      <pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/post/tuebingen/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internship (UC Berkeley)</title>
      <link>https://VisionAI-lab.github.io/post/ucberkeley/</link>
      <pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/post/ucberkeley/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion</title>
      <link>https://VisionAI-lab.github.io/publication/tpos_iccv2023/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/tpos_iccv2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bridging the Domain Gap by Clustering-based Image-Text Graph Matching</title>
      <link>https://VisionAI-lab.github.io/publication/2023_icmlw_dg/</link>
      <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2023_icmlw_dg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Localization and Manipulation of Immoral Visual Cues for Safe Text-to-Image Generation</title>
      <link>https://VisionAI-lab.github.io/publication/2023_ethics_icmlw/</link>
      <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2023_ethics_icmlw/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion</title>
      <link>https://VisionAI-lab.github.io/publication/2023_icmlw_tpos/</link>
      <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/2023_icmlw_tpos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RUFI: Reducing Uncertainty in Behavior Prediction with Future Information</title>
      <link>https://VisionAI-lab.github.io/publication/rufi_cvprw2023/</link>
      <pubDate>Tue, 20 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/rufi_cvprw2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CloudNet: A LiDAR-Based Face Anti-Spoofing Model That Is Robust Against Light Variation</title>
      <link>https://VisionAI-lab.github.io/publication/cloudnet_ieeeaccess2023/</link>
      <pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/cloudnet_ieeeaccess2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Embedding-Dynamic Approach to Self-supervised Learning</title>
      <link>https://VisionAI-lab.github.io/publication/bmwreg_wacv/</link>
      <pubDate>Wed, 04 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/bmwreg_wacv/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Resolving Class Imbalance Problem for LiDAR-based Object Detector by Balanced Gradients and Contextual Ground Truth Sampling</title>
      <link>https://VisionAI-lab.github.io/publication/lidar_wacv2023/</link>
      <pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/lidar_wacv2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ORA3D Overlap Region Aware Multi-view 3D Object Detection</title>
      <link>https://VisionAI-lab.github.io/publication/ora3d/</link>
      <pubDate>Fri, 21 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/ora3d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Zero-shot Visual Commonsense Immorality Prediction</title>
      <link>https://VisionAI-lab.github.io/publication/ethics_bmvc/</link>
      <pubDate>Fri, 21 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/ethics_bmvc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bridging the Domain Gap towards Generalization in Automatic Colorization</title>
      <link>https://VisionAI-lab.github.io/publication/color_eccv2022/</link>
      <pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/color_eccv2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Grounding Visual Representations with Texts for Domain Generalization</title>
      <link>https://VisionAI-lab.github.io/publication/dgxai_eccv2022/</link>
      <pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/dgxai_eccv2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sound-guided Semantic Video Generation</title>
      <link>https://VisionAI-lab.github.io/publication/sound_eccv2022/</link>
      <pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/sound_eccv2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visiting Bay Area</title>
      <link>https://VisionAI-lab.github.io/post/bay/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/post/bay/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CVPR 2022 Workshop on Human-centered Intelligent Services Safety and Trustworthy</title>
      <link>https://VisionAI-lab.github.io/post/cvprw/</link>
      <pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/post/cvprw/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Zero-shot Visual Commonsense Immorality Prediction (Abstracted Version)</title>
      <link>https://VisionAI-lab.github.io/publication/ethics_cvpr/</link>
      <pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/ethics_cvpr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sound-Guided Semantic Image Manipulation</title>
      <link>https://VisionAI-lab.github.io/publication/sound2image3/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/sound2image3/</guid>
      <description></description>
    </item>
    
    <item>
      <title>StopNet: Scalable Trajectory and Occupancy Prediction for Urban Autonomous Driving</title>
      <link>https://VisionAI-lab.github.io/publication/stopnet/</link>
      <pubDate>Sat, 05 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/stopnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Occupancy Flow Fields for Motion Forecasting in Autonomous Driving</title>
      <link>https://VisionAI-lab.github.io/publication/occupancyflow/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/occupancyflow/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards Explainable and Advisable Model for Self-driving Cars</title>
      <link>https://VisionAI-lab.github.io/publication/ailetters/</link>
      <pubDate>Fri, 31 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/ailetters/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Scenario-Based Platform for Testing Autonomous Vehicle Behavior Prediction Models in Simulation</title>
      <link>https://VisionAI-lab.github.io/publication/scenic/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/scenic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Audio-Semantic Image Synthesis for Artistic Paintings</title>
      <link>https://VisionAI-lab.github.io/publication/sound2image/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/sound2image/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sound-guided Semantic Image Manipulation</title>
      <link>https://VisionAI-lab.github.io/publication/sound2image2/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/sound2image2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inter-domain curriculum learning for domain generalization</title>
      <link>https://VisionAI-lab.github.io/publication/ictexpress/</link>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/ictexpress/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SelfReg: Self-supervised Contrastive Regularization for Domain Generalization</title>
      <link>https://VisionAI-lab.github.io/publication/selfreg/</link>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/selfreg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BMWReg: Brownian-diffusive, Multiview, Whitening Regulararizations for Self-supervised Learning</title>
      <link>https://VisionAI-lab.github.io/publication/bmwreg/</link>
      <pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/bmwreg/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards an Interpretable Deep Driving Network by Attentional Bottleneck</title>
      <link>https://VisionAI-lab.github.io/publication/attnbot/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/publication/attnbot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://VisionAI-lab.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Project</title>
      <link>https://VisionAI-lab.github.io/project/example/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/project/example/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://VisionAI-lab.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://VisionAI-lab.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
